{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98199764-ad85-4a1b-82f3-bca3fe635f37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# **Players Table Root**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1274ba36-a709-4824-a1aa-6cd119e058d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Creating a backup of the players table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67019fc3-dfa6-4aea-9b8a-e0a6ba3c57c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_complete_bckup = spark.table('bronze.players_complete')\n",
    "players_complete_bckup.write.mode('overwrite').saveAsTable('bronze.players_complete_bckup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b78a8bd-3b64-458e-aa2b-af0dc7eac57c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Dataframe creation and update of the table players_complete in the bronze DB, using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdf32865-4a08-4a5b-ba2e-e00c0988ae5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_default = spark.table('current_month_players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd12ce8-7cf0-4534-92d9-f4e8404221a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "filename_current = players_default.agg(first(col('file_name'))).collect()[0][0]\n",
    "print(filename_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6eb74bb-618b-482a-b3a9-50508bd2feba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_df = spark.table('bronze.players_complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad499c55-f9c3-4aa4-b6de-33d9184fe770",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Performing a conditional check to avoid appending duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fb79fc-bb64-407b-97fd-7fe3f4ef63dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_df_checker = players_df.filter(col('file_name').isin(filename_current))\n",
    "if players_df_checker.isEmpty():\n",
    "    players_default.write.mode('append').saveAsTable('bronze.players_complete')\n",
    "    print('players_complete appended')\n",
    "else:\n",
    "    print('File_name already exists.')\n",
    "\n",
    "players_df = spark.table('bronze.players_complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d861721-3cbe-46b0-aa39-cad8b3b4ba41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Creating a function in Spark to extract and store the characters from indices 9 to 14 to create a new column with these stored characters, which will be my date column. This was only possible because all files came standardized with the month and year in these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356a8fb9-7383-458e-bde6-d45edb6778e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import StringType\n",
    "\n",
    "# Function to extract characters from indices 9 to 14\n",
    "\n",
    "extract_substring = F.udf(lambda x: x[9:14] if len(x) > 15 else x, StringType())\n",
    "\n",
    "# Apply the transformation to the column file_name\n",
    "players_df_updated = players_df.withColumn('rating_date', extract_substring(F.col('file_name')))\n",
    "\n",
    "# Display the result\n",
    "display(players_df_updated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad03b17-1bf5-4aeb-88d1-41a66a4b7fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Checking the created column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f30925-0ca7-4d7c-9151-a68fd90108e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(players_df_updated.groupBy('rating_date').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd1dae18-862b-425d-a738-cf443452e317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_df_updated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dccb7f73-3978-4199-8f55-d3537e672461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Filtering and transforming data, focusing on converting columns to appropriate types and formatting dates for analysis or display, checking the data quality of the fideid column, keeping only fideids >=6 and <=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510c0a61-3771-4e55-9232-1d10fc151880",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, length\n",
    "\n",
    "players_transform_type = players_df_updated.filter(col('fideid').isNotNull()) \\\n",
    "    .withColumn('fideid', col('fideid').cast('integer')) \\\n",
    "    .withColumn('rating_date', to_date(col('rating_date'), 'MMMyy')) \\\n",
    "    .withColumn('birthday', col('birthday').cast('integer')) \\\n",
    "    .withColumn('rating', col('rating').cast('integer')) \\\n",
    "    .withColumn('fideid_length', length(col('fideid').cast('string')))\n",
    "\n",
    "players_transform_type2 = players_transform_type.filter((col('fideid_length') >= 6) & (col('fideid_length') <= 9))\n",
    "\n",
    "players_transform_type = players_transform_type2.drop('fideid_length')\n",
    "\n",
    "display(players_transform_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cde245c-d69f-43d4-99fa-434f52e03333",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Changing the date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b9a919-c50c-4322-bb9e-0a36f5206277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "players_transform_type = players_transform_type.withColumn('rating_date', date_format(col('rating_date'), 'MMM/yyyy'))\n",
    "\n",
    "display(players_transform_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22fb5476-7b96-40cc-a7c7-8a019b6d620d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Checking the length of entries in the country and sex columns, checking if the rating column have any entry over 9.999 and excluding anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091d48af-6860-426e-86a0-392cba9dec92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if players_transform_type.filter(length('country') != 3).isEmpty():\n",
    "    print('All countries checked, with no errors')\n",
    "    \n",
    "else:\n",
    "    players_transform_type = players_transform_type.filter(length('country') == 3)\n",
    "    players_errors = players_transform_type.filter(length('country') != 3).count()\n",
    "    print(f'There was {players_errors} errors.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe1697f-0e34-4d02-9e69-2e0e1c9b40b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if players_transform_type.filter(length('sex') != 1).isEmpty():\n",
    "    print('All rows checked, with no errors')\n",
    "    \n",
    "else:\n",
    "    players_transform_type = players_transform_type.filter(length('sex') == 1)\n",
    "    players_errors = players_transform_type.filter(length('sex') != 1).count()\n",
    "    print(f'There was {players_errors} errors.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "111c5b12-6367-4da9-9e20-7ca6fa5b20b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if players_transform_type.filter((col('rating') >9999)|(col('rating') <0)).isEmpty():\n",
    "    print('All rows checked, with no errors')\n",
    "\n",
    "else:\n",
    "    players_transform_type = players_transform_type.filter((col('rating') <9999 & (col('rating') >0)))\n",
    "    players_errors = players_transform_type.filter((col('rating') >9999)|(col('rating') <0)).count()\n",
    "    print(f'There was {players_errors} errors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a96f5ebe-7a08-44c6-97ad-dc32844577b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Checking if when sex = F it represents a male title, while counting and replacing the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5f7693-28bf-4cb2-b04e-e7d62b6bc779",
     "showTitle": true,
     "title": "CORRECTING TITLES - FEMALE"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize a counter\n",
    "substitution_counter = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Define a UDF to count the substitutions\n",
    "def count_substitution(title, sex):\n",
    "    global substitution_counter\n",
    "    if sex == 'F' and title in ['GM', 'IM', 'FM', 'CM']:\n",
    "        substitution_counter.add(1)\n",
    "    return title\n",
    "\n",
    "count_substitution_udf = udf(count_substitution, StringType())\n",
    "\n",
    "# Apply the substitutions and count\n",
    "players_transform_type2 = players_transform_type.withColumn(\n",
    "    'title',\n",
    "    when((col('sex') == 'F') & (col('title') == 'GM'), 'WGM')\n",
    "    .when((col('sex') == 'F') & (col('title') == 'IM'), 'WIM')\n",
    "    .when((col('sex') == 'F') & (col('title') == 'FM'), 'WFM')\n",
    "    .when((col('sex') == 'F') & (col('title') == 'CM'), 'WCM')\n",
    "    .otherwise(col('title'))\n",
    ")\n",
    "\n",
    "# Apply the UDF to count the substitutions\n",
    "players_transform_type3 = players_transform_type2.withColumn(\n",
    "    'title', count_substitution_udf(col('title'), col('sex'))\n",
    ")\n",
    "\n",
    "# Print the substitution counter\n",
    "print(f\"Substitution counter = {substitution_counter.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4611b468-ab24-4931-9f98-38c426c1ef1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Checking if when sex = M it represents a female title, while counting and replacing the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5665ca4d-9164-4554-89ec-3d3f2b54751a",
     "showTitle": true,
     "title": "CORRECTING TITLES - MALE"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize a counter\n",
    "substitution_counter2 = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Define a UDF to count the substitutions\n",
    "def count_substitution(title, sex):\n",
    "    global substitution_counter2\n",
    "    if sex == 'M' and title in ['WGM', 'WIM', 'WFM', 'WCM']:\n",
    "        substitution_counter2.add(1)\n",
    "    return title\n",
    "\n",
    "count_substitution_udf2 = udf(count_substitution, StringType())\n",
    "\n",
    "# Apply the substitutions and count\n",
    "players_transform_type3 = players_transform_type3.withColumn(\n",
    "    'title',\n",
    "    when((col('sex') == 'M') & (col('title') == 'WGM'), 'GM')\n",
    "    .when((col('sex') == 'M') & (col('title') == 'WIM'), 'IM')\n",
    "    .when((col('sex') == 'M') & (col('title') == 'WFM'), 'FM')\n",
    "    .when((col('sex') == 'M') & (col('title') == 'WCM'), 'CM')\n",
    "    .otherwise(col('title'))\n",
    ")\n",
    "\n",
    "# Apply the UDF to count the substitutions\n",
    "players_transform_type4 = players_transform_type3.withColumn(\n",
    "    'title', count_substitution_udf2(col('title'), col('sex'))\n",
    ")\n",
    "\n",
    "# Print the substitution counter\n",
    "print(f\"Substitution counter = {substitution_counter2.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "504f50f6-a2c5-4541-b37f-d79bbdcd59ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Checking the flags with the sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc9bd4db-23eb-451a-80b6-cec857c0247c",
     "showTitle": true,
     "title": "CORRECTING FLAGS - FEMALE"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "substitution_counter_flag = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def count_flag_substitution(flag, sex):\n",
    "    global substitution_counter_flag\n",
    "    if sex == 'F' and flag == 'I':\n",
    "        substitution_counter_flag.add(1)\n",
    "        return 'WI'\n",
    "    return flag\n",
    "\n",
    "count_flag_substitution_udf = udf(count_flag_substitution, StringType())\n",
    "\n",
    "\n",
    "players_transform_flag = players_transform_type4.withColumn(\n",
    "    'flag', count_flag_substitution_udf(col('flag'), col('sex'))\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Substitution counter = {substitution_counter_flag.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e15a335a-c53e-40dc-989c-c2c369558ce4",
     "showTitle": true,
     "title": "CORRECTING FLAGS - MALE"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "substitution_counter_flag = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def count_flag_substitution(flag, sex):\n",
    "    global substitution_counter_flag\n",
    "    if sex == 'M' and flag == 'WI':\n",
    "        substitution_counter_flag.add(1)\n",
    "        return 'I'\n",
    "    return flag\n",
    "\n",
    "count_flag_substitution_udf = udf(count_flag_substitution, StringType())\n",
    "\n",
    "\n",
    "players_transform_flag = players_transform_type.withColumn(\n",
    "    'flag', count_flag_substitution_udf(col('flag'), col('sex'))\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Substitution counter = {substitution_counter_flag.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a9a691-bf88-42ca-a9b0-4db46fa585b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Reordering the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f4c688d-b7ba-4dd0-8f26-262b59d59247",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reorder the columns to place the last column as the first column\n",
    "columns = players_transform_flag.columns\n",
    "reordered_columns = [columns[-1]] + columns[:-1]\n",
    "\n",
    "# Select the columns in the new order\n",
    "players_transform_flag_reordered = players_transform_flag.select(reordered_columns)\n",
    "\n",
    "display(players_transform_flag_reordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bfa63d-8646-45d7-9e76-6b87634581e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Creating a new column by concatenating 'rating_date' ande 'fideid', for a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e223d12b-a993-452b-b289-f2f5f298903b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Create a new column by concatenating 'rating_date' and 'fideid'\n",
    "players_transform_flag_reordered = players_transform_flag_reordered.withColumn('date_id_pk', concat_ws('_', 'rating_date', 'fideid'))\n",
    "\n",
    "# Reorder the columns to place the new column as the first column\n",
    "columns = players_transform_flag_reordered.columns\n",
    "reordered_columns = ['date_id_pk'] + columns[:-1]\n",
    "\n",
    "# Select the columns in the new order\n",
    "players_final = players_transform_flag_reordered.select(reordered_columns)\n",
    "\n",
    "display(players_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d38dbf8b-9171-4ff9-88a7-46eaaef1436e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Cleaning the non-usable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "091533d7-dd99-4960-bb14-16ad3966c2b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the specified columns from the DataFrame\n",
    "columns_to_drop = ['w_title', 'games', 'k', 'rapid_rating', 'rapid_games', 'rapid_k', 'blitz_rating', 'blitz_games', 'blitz_k', 'rating_date_fideid']\n",
    "players_final2 = players_final.drop(*columns_to_drop)\n",
    "\n",
    "display(players_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e4fec00-d484-459b-875b-809ffac7784d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Transforming the date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf95fbf7-4591-47a7-82ac-b2e36e611ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Transform the 'rating_date' column to date format and then format it as 'MMM/yyyy'\n",
    "players_final2 = players_final2.withColumn('rating_date', date_format(to_date('rating_date', 'MMM/yyyy'), 'MMM/yyyy'))\n",
    "\n",
    "display(players_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5312d483-7b3c-464f-8f51-b8283f6128d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Overwriting the players table in the silver's DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98670b0-6acc-404d-b9eb-2ff5a81ebb78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "players_final2.write.mode('overwrite').saveAsTable('silver.players')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver - players root",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
